# 28/05/2018

# https://www.coursera.org/learn/deep-neural-network/lecture/XzSSa/numerical-approximation-of-gradients
two side approximation is more accurate than the one side approx

# https://www.coursera.org/learn/deep-neural-network/lecture/htA0l/gradient-checking
gradient check
    reshape the parameters 
    d_parameters 
        into big vector
        do numercial approx 
        on each dimension
        to get theta_approx
        
    check the approximationality between theta_approx and theta
        use cos similarity
            normalization with the length of theta_approx and theta
            
https://www.coursera.org/learn/deep-neural-network/lecture/6igIc/gradient-checking-implementation-notes
    with drop out the J is different as the network is different
    then, the gradient checking technique is not suitable to compute here
    so, do gradient checking first, when find no problem, turn on the drop out
    
xavier initialization
    used for making the wx not too big or too small, so that the g(wx) has the big slope, then make the optimazation faster,
        tanh : np.sqrt(1 / n)
            # xavier
        ReLU : np.sqrt(2 / n)

    
# https://www.coursera.org/learn/deep-neural-network/lecture/qcogH/mini-batch-gradient-descent
    an epoch
        single pass through the training set

# https://www.coursera.org/learn/deep-neural-network/lecture/lBXu8/understanding-mini-batch-gradient-descent
    stochastic gradient descent
        mini batch size == 1
        
    typical batch size
        2**n
            64 ~ 512
            
# https://www.coursera.org/learn/deep-neural-network/lecture/duStO/exponentially-weighted-averages
    moving average
        exponentially weighted average, 
        reason it called "exponentially", is because the "feedback" structure, exponential filter
        
        
        
        
# https://www.coursera.org/learn/deep-neural-network/lecture/y0m1f/gradient-descent-with-momentum
    using exponentially weighted average to build a better optimization algorithm
        calculating the exponentially weight of the gradient, 
        use this gradient to update the gradients
        
    reduce the speed onsome specific direction
    
    momentum, 
        # df: use the previous speed direction as component of current one, once find a direction, keep go along it for some extent
        here is using average to smooth directions,
            so that direction with more ocileration is slown down
            the direction with less ocilleration is fasteren up
            
            
# https://www.coursera.org/learn/deep-neural-network/lecture/BhJlm/rmsprop
    RMSprop
        root mean squre properation
        not using the S_w directly
        
        
# https://www.coursera.org/learn/deep-neural-network/lecture/w9VCZ/adam-optimization-algorithm
    adam
        adaptive moment estimation
            = momentum + RMSprop
                # divide the momentum by the RMSprop
        
        
# https://www.coursera.org/learn/deep-neural-network/lecture/RFANA/the-problem-of-local-optima
    momentum
    RMSprop
    Adam
        are all for speed up the algorithm, by allowing to use the big learning rate
        which is important to overcome the plateau problem
        
        
# https://www.coursera.org/learn/deep-neural-network/lecture/3rdqN/using-an-appropriate-scale-to-pick-hyperparameters
    sample in log scale
        r = -4 * np.random.uniform()
        alpha = 10**r
        
        
# https://www.coursera.org/learn/deep-neural-network/lecture/4ptp2/normalizing-activations-in-a-network
    batch normalization
        # local nomalization?
        make it easier for hyperparameters search
        
        not for normalization a mini batch
        instead it normalize the z
        
# https://www.coursera.org/learn/deep-neural-network/lecture/RN8bN/fitting-batch-norm-into-a-neural-network
    reason called batch norm
        using with the mini batch, instead of the batch gradient
        but in fact no difference of the mechanism, only using the different mean and variance
        
        
        
        
# 29/05/2018
# https://www.coursera.org/learn/deep-neural-network/lecture/81oTm/why-does-batch-norm-work
    batch norm reduce the previous layers parameters changes leading to the current activation units change too much, 
    then the performance of the NN will be impacted, as the new distribution is not familiar to the NN
    
    boost the independence between different layers, speed up the learning speed? how?
        # so as to have bigger slope of the g

    add noise to hidden layers, make the dependence weaker, not strongly rely on some specific hidden unit,
    thus slight regularization effect
    
        as bigger mini batch size lead to smaller noise on mean and var,
        thus wierd:
            bigger batch weaker regularization effect


# https://www.coursera.org/learn/deep-neural-network/lecture/FsoNw/batch-norm-at-test-time
    problem with my TP.predict
        using half of the batch normalization
        but not applying the normalization to the prediction?0
        
        
        
# https://www.coursera.org/learn/deep-neural-network/lecture/HRy7y/softmax-regression
    why not just trivial logistic regression, and normalize among all the output units
    softmax is function of vector, instead of sigle value like ReLU or logistic regression


# https://www.coursera.org/learn/deep-neural-network/lecture/LCsCH/training-a-softmax-classifier
    hardmax
        max with 1,
        others with 0
        
    when c == 2, softmax == logistic
        
    softmax loss function does not use (1-y)log y  to deal with the negative example, 
    is because, there is the constraint on sum(Y) = 1, as long as the prob corresponding to 1
    is big, the prob corresponding to 0 is small




# 
XOR XOR XOR ...
    2**(n-1)
    due to n-1 * XOR, can be 0/1
    then 2**(n-1) sequence of "111011101101"(or) union together to enumerate all the possible sequence
    of XOR sequence
    
    
# https://www.coursera.org/learn/machine-learning-projects/lecture/LG12R/avoidable-bias
    by definition, nothing can be better than the Bayes error, unless overfitting
    
    avoidable bias
    variance
        tools for analysising what kind of problem should be focus on,
        i.e. the one has more space for improvment
        
        
# https://www.coursera.org/learn/machine-learning-projects/lecture/IGRRb/cleaning-up-incorrectly-labeled-data
    systematic errors
    





# https://www.coursera.org/learn/machine-learning-projects/lecture/WNPap/transfer-learning
    pre-training 
    fine tunning
    
    why transfer learning minght not hearts?
        what if it already stuck on a pleatue by pre training, and fine tunning could not get it out?
    
    # more popular!
    
    
# https://www.coursera.org/learn/machine-learning-projects/lecture/l9zia/multi-task-learning
    the main difference between softmax and multi-task
        multi-task is using the combination of logistic regressions,
            1 image multi-labels
            allow multiple 1 in output label
            when the images are not fully labeled, denote the unchecked label dimension with "?",
            then at the final output we can just skip the "?"s when calculate the loss  
        softmax
            1 image sigle-label
            only allow sigle 1 in output label
            
            
both transfer learning and multi-task learning, it seems need much more auxiliary data than the explicit data,
why? will not lead to bias??
            
            
            
# https://www.coursera.org/learn/machine-learning-projects/lecture/k0Klk/what-is-end-to-end-deep-learning
    end to end learning
        is using one NN directly predict from a image to the label
        
        simple system structure, direct mapping
        need a lot of data
        
    divide up, into steps
        need much less data
    
    
    
    
    
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/o7CWi/padding
    padding
        is for solve the problems:
            1. shrinking output
            2. throw away info from edges
            
        valid
            no padding
        same
            maintain the same size between input image and output image
            
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/wfUhx/strided-convolutions
    stride convolution
        means each step shift not only one element
        
    
    
    
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/A9lXL/simple-convolutional-network-example
    general trend of convolutional network
        1. n_h n_w increasing
        2. n_c descreasing
        
    3 types of layers
        1. convolution
        2. pooling
        3. fully connected
        
        
        
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/hELHk/pooling-layers
    max pooling
        normal sliding window, but no params, just get the max
            # "max" feature
        emphasis on some particular feature
        
        used more often
        
    average pooling
        use to collapse the representation
        
    all poolings have no parameters to learn
        
        
        
        
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/uRYL1/cnn-example
    as pool layer does not have weights, 
    conventionally treat it as the same layer with the conv layer
        
    full connection layer
        normal NN layer
        
        units in adjecent layers are fully connected
        on the contrary, the CONV layers, the features only related to the features of the previous layer,
        which inside the sliding window
        
        
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/Xv7B5/why-convolutions
    conv uses much less params then FC
        due to 
            1. sliding window - parameter sharing
            2. only connect to sliding window
        
        translation invariance
            robust
            
            
"parameters"
    conv
        300 * 300 * 3
        5 * 5 * 3 + 1
            parameters for one filter, need to be 3-D
        (5 * 5 * 3 + 1) * 100
        
    NN
        300 * 200 + 300
            300 weights for the bias 1 : b_1 ... b_300
        parameters 100 filters
        
        
        
        
        
        
# 31/05/2018
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/HAhz9/resnets
    residual network
        skip connection / short cut
            a[l] skip a[l+1] and sum with z[l+1] directly
            
        helps with the vanishing and exploding gredient problem
        # https://www.coursera.org/learn/convolutional-neural-networks/lecture/XAKNO/why-resnets-work
            identity function is easy for residul block to learn

        
        
        
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/5WIZm/inception-network-motivation
    error on 2:06
        pooling conduct on each channel independently, does not change the depth of the image
        other thing is just like the normal convolution sliding window
        
        
        
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/5WIZm/inception-network-motivation
    bottleneck layer
        use the 1*1 filter to shrink the image firstly
        
        this process will not lossing the information?
        
        
        
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/4THzO/transfer-learning
    transfer learning
        freeze some layers
        train some layers
            based on how many data you have
            
            
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/AYzbX/data-augmentation
    not all machine learning app need a lot of data
    but computer vision does, cannot get enough data
    
    
    
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/nEeJM/object-localization
    localization
        1 object in the fig
    detection
        multi objects in the fig
        
    put the pc, x,y, dh, dw, dx, dy into the y,
    which in fact is a combination of classification and regression problem.
        the loss is in different shape when pc has different value.
        "potential problem?:"
            only compare pc when pc == 0
            |y^, y| when pc != 0,
                is if possible, the |y^,y| is bigger than the |pc^, pc|
                thus, cost put more weight on the |y^,y|, 
                so that has worse performance on |pc^,pc|?
        
        
        
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/6UnU4/convolutional-implementation-of-sliding-windows
    fc 400 == 1*1 filter * 400
        # which is just the network in network
        
    make use of the inherient sliding window of convolution network, 
    to implement the window sliding in object detection

    the yellow margine does not have strictly correspondingness among different layer
    just stand for the area bigger than the one window version
    
    the pipline for processing one window, if apply it on the image,
    == applying the window to slide the image.

    this is infact use a pipline of filters to equal to a single of filters
    can we directly check if it is equivalent?
    

    
    
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/9EcTO/bounding-box-predictions
    the YOLO algorithm is intuitively set what we want, so as to set the shape of output,
    how can we be sure the output is just what we want??? 
    as there are mix and non-linear procedure among the procedure
        # as using ReLU?? 
        all the mysteries are in the labeling the dataset
        
    different tasks / algorithms
        object localization
            directly predict using CNN
        landmark detection
            directly predict using CNN
            
            
        object detect
            # detection algo 1
            sliding window
            or using CNN all windows in one go
        bounding box prediction
            # detection algo 2
            fixed window combine with obj localization
                # not using sliding window, so that call YOLO - you only look once


        
        
        
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/p9gxz/intersection-over-union
    intersection over union
        map localisation to accuracy, its a metric
        also a metric of measuring how similar to boxes are
        
        
        
        
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/dvrjH/non-max-suppression
    Non-max suppression algorithm
        based on the fix grid, YOLO like aogorithm
        the localization bounding grid is not the fix meshgrid, which could have arbitrary size
        
    
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/fF3O0/yolo-algorithm
    when conduct training, 
    the task is to form the training set, which contains manually associate the object to the bounding box,
    i.e. manually set the vector value, 
    
    when predicting, the algorithm will predict values for both bounding box, i.e. give contents to the whole vector,
    then use the non-max suppression to choose the one with the big pc
    

    # https://www.coursera.org/learn/convolutional-neural-networks/lecture/6UnU4/convolutional-implementation-of-sliding-windows
        YOLO is using the fix meshgrid, which is just the kind of sliding window without the overlap, 
        that is which can be implemented by the conv network
    
        the stride is control by the MAX POOL size! 10:03
        "overfeat:integrated recognition, localization and detection using convolutional networks"
        
        so in order to ensure no overlap, we need to set the MAX POOL size manually?




# https://www.coursera.org/learn/nlp-sequence-models/lecture/aJT8i/notation
    the sequence traning samples have the different length, 
    the original NN CNN deal with the sample with the identical size,
        these are the fundamental difference
        
        
# https://www.coursera.org/learn/nlp-sequence-models/lecture/ftkzt/recurrent-neural-network-model
    1. NN cannot deal with input ouput having different size, except using maximum size of the sequence and use padding
    2. NN doesnt share features learned accross different positions of text
        no knowledge generalization within the sequence, as all in one go?
        how is the sharing be represented in CNN?
            use one filter scan through the whole figure, which is trying to extract out one feature, may be complex and compound
            but "scan through" is the generalization
            
            
            
# https://www.coursera.org/learn/nlp-sequence-models/lecture/ftkzt/recurrent-neural-network-model
    RNN is making use of previous activation a<t-1> to compute the current activation a<t> 
        a<t> recurrent
        
        
# https://www.coursera.org/learn/nlp-sequence-models/lecture/bc7ED/backpropagation-through-time
    wa ba wy by are all shared by different t
    as it is indeed the same network, but recurrent the a<t-1>
    
    R - CNN : region proposal CNN, not RNN
    RNN : recurrent NN
    
    
    
 # https://www.coursera.org/learn/nlp-sequence-models/lecture/PKMRR/vanishing-gradients-with-rnns
    gradient exploding - gradient clipping solve
        bigger than some threshold, rescale
        
        
        
# https://www.coursera.org/learn/nlp-sequence-models/lecture/agZiL/gated-recurrent-unit-gru
    gradient vanishing - gated recurrent unit (GRU)
    GRU & LSTM


# https://www.coursera.org/learn/nlp-sequence-models/lecture/KXoay/long-short-term-memory-lstm
    in GRU at == ct
    in LSTM at != ct
    
    
# https://www.coursera.org/learn/nlp-sequence-models/lecture/fyXnn/bidirectional-rnn
    bidirectinal RNN - BRNN
        forward propagation on both directions, left to right + right to left
