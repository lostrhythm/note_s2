week 2
logistic regressoin gradient descent
    computation graph approach
    
    g = 1 / (1 + exp(-z))
    dg / dz = g(1-g)
    
    
code conventions
    dz = dj / dz
        # instead of the difference(z)
    dvar = dj / dvar
    
    
# https://www.coursera.org/learn/neural-networks-deep-learning/lecture/NYnog/vectorization
    by default using the column vector
    z = np.dot(w,x)
        # vector multiply
        
# https://www.coursera.org/learn/neural-networks-deep-learning/lecture/moUlO/vectorizing-logistic-regression
    by default, samples are in columns in data set
        X.shape = (n, m)
            # m is the amount of samples
            
# https://www.coursera.org/learn/neural-networks-deep-learning/lecture/IgFnJ/vectorizing-logistic-regressions-gradient-output
    the cumulative of the matrix of the gradient does not need to use for loop as well
        as 
            col * row = matrix
            [col, col, col] @ [row, row, row].T = sum(col * row)
                # see it as the row of columns, thus the "summarization" is implied
                # Time 8:27, the final equations
                
                
                
A trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b * c * d, a) is to use:
    X_flatten = X.reshape(X.shape[0], -1).T      # X.T is the transpose of X
    
    
    
    
# https://www.coursera.org/learn/neural-networks-deep-learning/lecture/qg83v/neural-networks-overview
    computation graph is different from the NN representation graph
        the input is x w b, instead of x_1 x_2 x_3 ...
        
        
        
        
# https://www.coursera.org/learn/neural-networks-deep-learning/lecture/tyAGh/computing-a-neural-networks-output
    w_i is column vector
    W == stack(w_1.T, w_2.T, w_3.T, ...)
    
    
# https://www.coursera.org/learn/neural-networks-deep-learning/lecture/4dDC1/activation-functions
    tanh is almost works better than the sigmoid function
    as the activation value has a nearly 0 mean
    but sigmoid is still being used in the output layer, for want the output in (0,1)
    
    when the z is very small or very large, the tanh and sigmoid will both have very small slope, 
    thus slow down the gradient descent
    
    "ReLU"
        Rectified linear unit
        max(0, z)
    
    leaky ReLU
        small slope when z<0
        
        
# https://www.coursera.org/learn/neural-networks-deep-learning/lecture/OASKH/why-do-you-need-non-linear-activation-functions
    if using linear activation function and sigmoid function in output layer,
    no matter how many layers it has, no more expressive than no hidden layer
    
    the only place might use a linear activation function,
    is when doing regressoin, set the hidden layer activation units as tanh or ReLU,
    but the output layer as the linear function, thus the y could be (-inf, inf)
    
    
    
    
# https://www.coursera.org/learn/neural-networks-deep-learning/lecture/OASKH/why-do-you-need-non-linear-activation-functions
    g = tanh
    g' = 1 - g**2
    
    
    g = max(0,z)
    g' =  
        0   z<0
        1   z>=0
            # implementation approximation, in fact should be undefined on z == 0
            
            
            
# https://www.coursera.org/learn/neural-networks-deep-learning/lecture/Wh8NI/gradient-descent-for-neural-networks
    np.sum(..., keepdims = True)
        prevent (n,1) turning to (n,)
        
    b 是跟 bias == 1 的维度相乘的项， (统一在W中的话， 即 W 的第一列，) 即 bias
    
# https://www.coursera.org/learn/neural-networks-deep-learning/lecture/6dDj7/backpropagation-intuition-optional
    # Time 14:23 vectorization of back propagation
    
    
# https://www.coursera.org/learn/neural-networks-deep-learning/lecture/XtFPI/random-initialization
    w = np.random.random((2,2)) * 0.001
        initialize small, so that will not be at the far end of the tanh where has small slope
        
        
        
        
        
        
        
        
# 25/05/2018
# https://www.coursera.org/learn/deep-neural-network/lecture/cxG1s/train-dev-test-sets
    1. big data data set split:
        98 1 1
    2. dev/test set need to be same distribution
    
# https://www.coursera.org/learn/deep-neural-network/lecture/ZhclI/bias-variance
    high bias + high variance
        局部 high bias
        局部 high variance 
        同时发生
        
# https://www.coursera.org/learn/deep-neural-network/lecture/ZBkx4/basic-recipe-for-machine-learning
    with bigger network and more data
    we can achieve the low bias and low variance at the same time
    
    
# https://www.coursera.org/learn/deep-neural-network/lecture/Srsrc/regularization
    Frobenius norm, L2 norm of matrix
    
    
# https://www.coursera.org/learn/deep-neural-network/lecture/Srsrc/regularization
    penalization, could be regarded as 
        large lambda -> smaller w -> smaller z -> linear activation function (in tanh) -> linear network -> cannot fit very complex boudnary
        
# https://www.coursera.org/learn/deep-neural-network/lecture/eM33A/dropout-regularization
    invert drop out
        1. mask = np.random.random(a.shape[0], a.shape[1]) < keep-prob
        2. a = a * mask
        3. a /= keep-prob
    一部分 a 设为 0 了， 那么其对应的 w 的 grad 就为0 也就是在这一步里 不改变 w
    keep-prob can be different on different layers
    
    
    
    
# 27/05/2018
# https://www.coursera.org/learn/deep-neural-network/lecture/C9iQO/vanishing-exploding-gradients
    activation value 
        vanishing
        exploding 
            due to the deepness of layers
            which will lead to the tiny or large change of the gradients
            
            
# https://www.coursera.org/learn/deep-neural-network/lecture/RwqYe/weight-initialization-for-deep-networks
    using tanh
        np.random.randn() * np.sqrt(1/n)
            # n is the num of activation units of one layers
            change the variance from 1 to 1/n
                # this works as the np.sqrt(100) is on x axis
                
    using ReLU
        2 / n

# 28/05/2018

# https://www.coursera.org/learn/deep-neural-network/lecture/XzSSa/numerical-approximation-of-gradients
two side approximation is more accurate than the one side approx

# https://www.coursera.org/learn/deep-neural-network/lecture/htA0l/gradient-checking
gradient check
    reshape the parameters 
    d_parameters 
        into big vector
        do numercial approx 
        on each dimension
        to get theta_approx
        
    check the approximationality between theta_approx and theta
        use cos similarity
            normalization with the length of theta_approx and theta
            
https://www.coursera.org/learn/deep-neural-network/lecture/6igIc/gradient-checking-implementation-notes
    with drop out the J is different as the network is different
    then, the gradient checking technique is not suitable to compute here
    so, do gradient checking first, when find no problem, turn on the drop out
    
xavier initialization
    used for making the wx not too big or too small, so that the g(wx) has the big slope, then make the optimazation faster,
        tanh : np.sqrt(1 / n)
            # xavier
        ReLU : np.sqrt(2 / n)

    
# https://www.coursera.org/learn/deep-neural-network/lecture/qcogH/mini-batch-gradient-descent
    an epoch
        single pass through the training set

# https://www.coursera.org/learn/deep-neural-network/lecture/lBXu8/understanding-mini-batch-gradient-descent
    stochastic gradient descent
        mini batch size == 1
        
    typical batch size
        2**n
            64 ~ 512
            
# https://www.coursera.org/learn/deep-neural-network/lecture/duStO/exponentially-weighted-averages
    moving average
        exponentially weighted average, 
        reason it called "exponentially", is because the "feedback" structure, exponential filter
        
        
        
        
# https://www.coursera.org/learn/deep-neural-network/lecture/y0m1f/gradient-descent-with-momentum
    using exponentially weighted average to build a better optimization algorithm
        calculating the exponentially weight of the gradient, 
        use this gradient to update the gradients
        
    reduce the speed onsome specific direction
    
    momentum, 
        # df: use the previous speed direction as component of current one, once find a direction, keep go along it for some extent
        here is using average to smooth directions,
            so that direction with more ocileration is slown down
            the direction with less ocilleration is fasteren up
            
            
# https://www.coursera.org/learn/deep-neural-network/lecture/BhJlm/rmsprop
    RMSprop
        root mean squre properation
        not using the S_w directly
        
        
# https://www.coursera.org/learn/deep-neural-network/lecture/w9VCZ/adam-optimization-algorithm
    adam
        adaptive moment estimation
            = momentum + RMSprop
                # divide the momentum by the RMSprop
        
        
# https://www.coursera.org/learn/deep-neural-network/lecture/RFANA/the-problem-of-local-optima
    momentum
    RMSprop
    Adam
        are all for speed up the algorithm, by allowing to use the big learning rate
        which is important to overcome the plateau problem
        
        
# https://www.coursera.org/learn/deep-neural-network/lecture/3rdqN/using-an-appropriate-scale-to-pick-hyperparameters
    sample in log scale
        r = -4 * np.random.uniform()
        alpha = 10**r
        
        
# https://www.coursera.org/learn/deep-neural-network/lecture/4ptp2/normalizing-activations-in-a-network
    batch normalization
        # local nomalization?
        make it easier for hyperparameters search
        
        not for normalization a mini batch
        instead it normalize the z
        
# https://www.coursera.org/learn/deep-neural-network/lecture/RN8bN/fitting-batch-norm-into-a-neural-network
    reason called batch norm
        using with the mini batch, instead of the batch gradient
        but in fact no difference of the mechanism, only using the different mean and variance
        
        
        
        
# 29/05/2018
# https://www.coursera.org/learn/deep-neural-network/lecture/81oTm/why-does-batch-norm-work
    batch norm reduce the previous layers parameters changes leading to the current activation units change too much, 
    then the performance of the NN will be impacted, as the new distribution is not familiar to the NN
    
    boost the independence between different layers, speed up the learning speed? how?
        # so as to have bigger slope of the g

    add noise to hidden layers, make the dependence weaker, not strongly rely on some specific hidden unit,
    thus slight regularization effect
    
        as bigger mini batch size lead to smaller noise on mean and var,
        thus wierd:
            bigger batch weaker regularization effect


# https://www.coursera.org/learn/deep-neural-network/lecture/FsoNw/batch-norm-at-test-time
    problem with my TP.predict
        using half of the batch normalization
        but not applying the normalization to the prediction?0
        
        
        
# https://www.coursera.org/learn/deep-neural-network/lecture/HRy7y/softmax-regression
    why not just trivial logistic regression, and normalize among all the output units
    softmax is function of vector, instead of sigle value like ReLU or logistic regression


# https://www.coursera.org/learn/deep-neural-network/lecture/LCsCH/training-a-softmax-classifier
    hardmax
        max with 1,
        others with 0
        
    when c == 2, softmax == logistic
        
    softmax loss function does not use (1-y)log y  to deal with the negative example, 
    is because, there is the constraint on sum(Y) = 1, as long as the prob corresponding to 1
    is big, the prob corresponding to 0 is small




# 
XOR XOR XOR ...
    2**(n-1)
    due to n-1 * XOR, can be 0/1
    then 2**(n-1) sequence of "111011101101"(or) union together to enumerate all the possible sequence
    of XOR sequence
    
    
# https://www.coursera.org/learn/machine-learning-projects/lecture/LG12R/avoidable-bias
    by definition, nothing can be better than the Bayes error, unless overfitting
    
    avoidable bias
    variance
        tools for analysising what kind of problem should be focus on,
        i.e. the one has more space for improvment
        
        
# https://www.coursera.org/learn/machine-learning-projects/lecture/IGRRb/cleaning-up-incorrectly-labeled-data
    systematic errors
    





# https://www.coursera.org/learn/machine-learning-projects/lecture/WNPap/transfer-learning
    pre-training 
    fine tunning
    
    why transfer learning minght not hearts?
        what if it already stuck on a pleatue by pre training, and fine tunning could not get it out?
    
    # more popular!
    
    
# https://www.coursera.org/learn/machine-learning-projects/lecture/l9zia/multi-task-learning
    the main difference between softmax and multi-task
        multi-task is using the combination of logistic regressions,
            1 image multi-labels
            allow multiple 1 in output label
            when the images are not fully labeled, denote the unchecked label dimension with "?",
            then at the final output we can just skip the "?"s when calculate the loss  
        softmax
            1 image sigle-label
            only allow sigle 1 in output label
            
            
both transfer learning and multi-task learning, it seems need much more auxiliary data than the explicit data,
why? will not lead to bias??
            
            
            
# https://www.coursera.org/learn/machine-learning-projects/lecture/k0Klk/what-is-end-to-end-deep-learning
    end to end learning
        is using one NN directly predict from a image to the label
        
        simple system structure, direct mapping
        need a lot of data
        
    divide up, into steps
        need much less data
    
    
    
    
    
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/o7CWi/padding
    padding
        is for solve the problems:
            1. shrinking output
            2. throw away info from edges
            
        valid
            no padding
        same
            maintain the same size between input image and output image
            
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/wfUhx/strided-convolutions
    stride convolution
        means each step shift not only one element
        
    
    
    
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/A9lXL/simple-convolutional-network-example
    general trend of convolutional network
        1. n_h n_w increasing
        2. n_c descreasing
        
    3 types of layers
        1. convolution
        2. pooling
        3. fully connected
        
        
        
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/hELHk/pooling-layers
    max pooling
        normal sliding window, but no params, just get the max
            # "max" feature
        emphasis on some particular feature
        
        used more often
        
    average pooling
        use to collapse the representation
        
    all poolings have no parameters to learn
        
        
        
        
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/uRYL1/cnn-example
    as pool layer does not have weights, 
    conventionally treat it as the same layer with the conv layer
        
    full connection layer
        normal NN layer
        
        units in adjecent layers are fully connected
        on the contrary, the CONV layers, the features only related to the features of the previous layer,
        which inside the sliding window
        
        
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/Xv7B5/why-convolutions
    conv uses much less params then FC
        due to 
            1. sliding window - parameter sharing
            2. only connect to sliding window
        
        translation invariance
            robust
            
            
"parameters"
    conv
        300 * 300 * 3
        5 * 5 * 3 + 1
            parameters for one filter, need to be 3-D
        (5 * 5 * 3 + 1) * 100
        
    NN
        300 * 200 + 300
            300 weights for the bias 1 : b_1 ... b_300
        parameters 100 filters
        
        
        
        
        
        
# 31/05/2018
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/HAhz9/resnets
    residual network
        skip connection / short cut
            a[l] skip a[l+1] and sum with z[l+1] directly
            
        helps with the vanishing and exploding gredient problem
        # https://www.coursera.org/learn/convolutional-neural-networks/lecture/XAKNO/why-resnets-work
            identity function is easy for residul block to learn

        
        
        
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/5WIZm/inception-network-motivation
    error on 2:06
        pooling conduct on each channel independently, does not change the depth of the image
        other thing is just like the normal convolution sliding window
        
        
        
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/5WIZm/inception-network-motivation
    bottleneck layer
        use the 1*1 filter to shrink the image firstly
        
        this process will not lossing the information?
        
        
        
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/4THzO/transfer-learning
    transfer learning
        freeze some layers
        train some layers
            based on how many data you have
            
            
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/AYzbX/data-augmentation
    not all machine learning app need a lot of data
    but computer vision does, cannot get enough data
    
    
    
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/nEeJM/object-localization
    localization
        1 object in the fig
    detection
        multi objects in the fig
        
    put the pc, x,y, dh, dw, dx, dy into the y,
    which in fact is a combination of classification and regression problem.
        the loss is in different shape when pc has different value.
        "potential problem?:"
            only compare pc when pc == 0
            |y^, y| when pc != 0,
                is if possible, the |y^,y| is bigger than the |pc^, pc|
                thus, cost put more weight on the |y^,y|, 
                so that has worse performance on |pc^,pc|?
        
        
        
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/6UnU4/convolutional-implementation-of-sliding-windows
    fc 400 == 1*1 filter * 400
        # which is just the network in network
        
    make use of the inherient sliding window of convolution network, 
    to implement the window sliding in object detection

    the yellow margine does not have strictly correspondingness among different layer
    just stand for the area bigger than the one window version
    
    the pipline for processing one window, if apply it on the image,
    == applying the window to slide the image.

    this is infact use a pipline of filters to equal to a single of filters
    can we directly check if it is equivalent?
    

    
    
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/9EcTO/bounding-box-predictions
    the YOLO algorithm is intuitively set what we want, so as to set the shape of output,
    how can we be sure the output is just what we want??? 
    as there are mix and non-linear procedure among the procedure
        # as using ReLU?? 
        all the mysteries are in the labeling the dataset
        
    different tasks / algorithms
        object localization
            directly predict using CNN
        landmark detection
            directly predict using CNN
            
            
        object detect
            # detection algo 1
            sliding window
            or using CNN all windows in one go
        bounding box prediction
            # detection algo 2
            fixed window combine with obj localization
                # not using sliding window, so that call YOLO - you only look once


        
        
        
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/p9gxz/intersection-over-union
    intersection over union
        map localisation to accuracy, its a metric
        also a metric of measuring how similar to boxes are
        
        
        
        
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/dvrjH/non-max-suppression
    Non-max suppression algorithm
        based on the fix grid, YOLO like aogorithm
        the localization bounding grid is not the fix meshgrid, which could have arbitrary size
        
    
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/fF3O0/yolo-algorithm
    when conduct training, 
    the task is to form the training set, which contains manually associate the object to the bounding box,
    i.e. manually set the vector value, 
    
    when predicting, the algorithm will predict values for both bounding box, i.e. give contents to the whole vector,
    then use the non-max suppression to choose the one with the big pc
    

    # https://www.coursera.org/learn/convolutional-neural-networks/lecture/6UnU4/convolutional-implementation-of-sliding-windows
        YOLO is using the fix meshgrid, which is just the kind of sliding window without the overlap, 
        that is which can be implemented by the conv network
    
        the stride is control by the MAX POOL size! 10:03
        "overfeat:integrated recognition, localization and detection using convolutional networks"
        
        so in order to ensure no overlap, we need to set the MAX POOL size manually?




# https://www.coursera.org/learn/nlp-sequence-models/lecture/aJT8i/notation
    the sequence traning samples have the different length, 
    the original NN CNN deal with the sample with the identical size,
        these are the fundamental difference
        
        
# https://www.coursera.org/learn/nlp-sequence-models/lecture/ftkzt/recurrent-neural-network-model
    1. NN cannot deal with input ouput having different size, except using maximum size of the sequence and use padding
    2. NN doesnt share features learned accross different positions of text
        no knowledge generalization within the sequence, as all in one go?
        how is the sharing be represented in CNN?
            use one filter scan through the whole figure, which is trying to extract out one feature, may be complex and compound
            but "scan through" is the generalization
            
            
            
# https://www.coursera.org/learn/nlp-sequence-models/lecture/ftkzt/recurrent-neural-network-model
    RNN is making use of previous activation a<t-1> to compute the current activation a<t> 
        a<t> recurrent
        
        
# https://www.coursera.org/learn/nlp-sequence-models/lecture/bc7ED/backpropagation-through-time
    wa ba wy by are all shared by different t
    as it is indeed the same network, but recurrent the a<t-1>
    
    R - CNN : region proposal CNN, not RNN
    RNN : recurrent NN
    
    
    
 # https://www.coursera.org/learn/nlp-sequence-models/lecture/PKMRR/vanishing-gradients-with-rnns
    gradient exploding - gradient clipping solve
        bigger than some threshold, rescale
        
        
        
# https://www.coursera.org/learn/nlp-sequence-models/lecture/agZiL/gated-recurrent-unit-gru
    gradient vanishing - gated recurrent unit (GRU)
    GRU & LSTM


# https://www.coursera.org/learn/nlp-sequence-models/lecture/KXoay/long-short-term-memory-lstm
    in GRU at == ct
    in LSTM at != ct
    
    
# https://www.coursera.org/learn/nlp-sequence-models/lecture/fyXnn/bidirectional-rnn
    bidirectinal RNN - BRNN
        forward propagation on both directions, left to right + right to left



# https://www.coursera.org/learn/convolutional-neural-networks/lecture/lUBYU/what-is-face-recognition
    face
        verification
        recognition
        
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/gjckG/one-shot-learning
    one shot 
        recognize a person with only one picture - only one training example
        
    instead of predict if a image is belong to some specific person(softmax)
    learn a similarity function, using with to calculate the difference of two figure, then choose the most similar one
        # in fact is the normal Euclidean distance or the cos distance
        # key part is get a good representation of the image, then based on which to calculate the distance
        
    use the representation of the last FC layer as the encoding of the fig
    
    siamese network, network for get the representation
        # two network with the identical parameters, siamese
        
    loss = norm(f(x1), f(x2))
        # use a pair of image as one sample to train the network once
        same backpropagation
        
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/HuUtN/triplet-loss
    learning the obj func
        triplet:
            anchor
            positive
            negative
            
            3 images
            
    trivial solutions   
        1. all f(x) == 0
        2. all f(x) the same
        
    + margin
   
    training set has to have multiple figures of the same person,
    but when application, the database could only have signle figure of one person (A)
    this is because, the training process is for generating a good representation
    
    
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/xTihv/face-verification-and-binary-classification
    not using triplet, instead use binary classification,
    easy to construct the training set?
    
    
    
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/xTihv/face-verification-and-binary-classification
    each patch comes from different sample
    why 9 ?
    
    
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/jOnEU/cost-function
    use cost function to indicate which direction to change the image.
    
    
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/CvHv6/content-cost-function
    calculate based on the shallower layers of CNN, it demands more detailed similarity
    calculate based on the deeper layers of CNN, it only demands the topic is identical, like both dogs
    
    similar with the face recognization approach, use the distance between two images to form the cost function?
    
    the generation is the a[l](g), which is tuned by changing the weight? so in order to change the figure, 
    in fact is still changing the network
    
    
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/AzcCW/style-cost-function
    define the "style" of an image as 
        correlation between activations across channels
    different channels of a intermediate layer, denote different features, corresponding to different filters
    
    correlation,
        denotes different features are occuring together or not together often
        
    J(S) is based on the style matrix, style matrix is based on the correlation 
        (which in fact is unnormalized corvariance, multiply the corresponding elements of different channel)
        
        
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/OkHJ3/1d-and-3d-generalizations
    1D 3D data are both having the channels
        2D 2*3*3 != 3D 2*3*3
        as 3D 2*3*3 is in fact 2*3*3*1
        
        
        
        
# https://www.coursera.org/learn/nlp-sequence-models/lecture/6Oq70/word-representation
    transfer learning and word embedding
        transfer learning not necessarily reuse the parameters of NN
        may be just make use of the pre-trained word embeddings
        
    word embedding is train on the fixed sized vocabulary, and only limit words has the new representation,
    in face recognization, any new figs could be represented as new representation.
    
    
# https://www.coursera.org/learn/nlp-sequence-models/lecture/S2mat/properties-of-word-embeddings
    the parallelogram relationship hold true only in the original space, instead of in the T-SNE space
        T-SNE is non-linear
        
        
# https://www.coursera.org/learn/nlp-sequence-models/lecture/K604Z/embedding-matrix
    word embeddings are learnt in a matrix E
    use look up function embedding layer to pull out the new representation from E, 
    instead of using the matrix multiplication
    
# https://www.coursera.org/learn/nlp-sequence-models/lecture/APM5s/learning-word-embeddings
    neural language model
        put the representation generation matrix into the network as well, 
        so that to make use of backprop to solve it.
        
    skip gram model
        nearby 1 word
        
    parameter multiply with the label, is similar to the w in the sigmoid function of logistic regression

    softmax is expensive
    
    # think: where are the context vector
    
    
# https://www.coursera.org/learn/nlp-sequence-models/lecture/Iwx0e/negative-sampling
    in each generation, sample k negative models.
    and use these k+1 samples to train the nc binary classifier, instead of the nc size softmax
    
    use power of frequency to model the zipf's law?


# https://www.coursera.org/learn/nlp-sequence-models/lecture/IxDTG/glove-word-vectors
    xij == xji only when in large window
    reason use 
        theta_t.T @ e_c 
        is because, theta_t is related to the output target word,
        e_c is the representation of the context word
        
    glove is using the average of two vectors?
    why?
    
    glove is model the t c vec of a word at the same time,
    use theta_i.T @ e_j 
        # i,j are of two different words
    to compare with the cooccurance, so that the best representation will be minimize the 
    log Xij
    
    
    
# https://www.coursera.org/learn/nlp-sequence-models/lecture/gw1Xw/language-model-and-sequence-generation
    the language model is forming a RNN to predict the probability 
    of all possible outcome based on the previous step with softmax,
        thus when a setence comes in, 
        RNN generate all the conditional probabilities of the sentence, 
        i.e. possibility and prediction word pair       

    既可以用来计算序列概率， 也可以用来预测下一个词。


 # https://www.coursera.org/learn/nlp-sequence-models/lecture/4EtHZ/beam-search
    beam search consider multiple alternatives, like 10 most likely terms

    in each step is get the top 3, instead of 3 - 9 - 27 exponentially grow up.
    make use of 3 instances of RNN to calculate the distribution of next word condition on the previous sequence


# https://www.coursera.org/learn/nlp-sequence-models/lecture/AkjG2/refinements-to-beam-search
    length normalization
        均衡长句子与短句子的概率， 长句子倾向于有小的概率， 这是由于 其是概率的乘积
    when choose the best sentence, calculate the normalized probability over all sentences, then choose
    the one has the highest probability.
    
    
# https://www.coursera.org/learn/nlp-sequence-models/lecture/kC2HD/bleu-score-optional
    bleu score
        bilingual evaluation understudy
        metric for evaluating the system which has multiple equally good translations
        
        pn = modified precision on n-gram
        bleu = exp(average(p1,p2,p3,p4))
        
        single value metric
        
        compare the machine translation with the references generated by people
        also can be used in figure caption generation
        
        
        
 # https://www.coursera.org/learn/nlp-sequence-models/lecture/lSwVa/attention-model
    attention model
        for partially translate the sentences, instead of memory all and translate in once,
        use bidirectional RNN and context and one directional RNN
        
    use a seperate small NN with input s<t-1> and a<t'> to predict the attention a<t,t'>




