# 28/05/2018

# https://www.coursera.org/learn/deep-neural-network/lecture/XzSSa/numerical-approximation-of-gradients
two side approximation is more accurate than the one side approx

# https://www.coursera.org/learn/deep-neural-network/lecture/htA0l/gradient-checking
gradient check
    reshape the parameters 
    d_parameters 
        into big vector
        do numercial approx 
        on each dimension
        to get theta_approx
        
    check the approximationality between theta_approx and theta
        use cos similarity
            normalization with the length of theta_approx and theta
            
https://www.coursera.org/learn/deep-neural-network/lecture/6igIc/gradient-checking-implementation-notes
    with drop out the J is different as the network is different
    then, the gradient checking technique is not suitable to compute here
    so, do gradient checking first, when find no problem, turn on the drop out
    
xavier initialization
    used for making the wx not too big or too small, so that the g(wx) has the big slope, then make the optimazation faster,
        tanh : np.sqrt(1 / n)
            # xavier
        ReLU : np.sqrt(2 / n)

    
# https://www.coursera.org/learn/deep-neural-network/lecture/qcogH/mini-batch-gradient-descent
    an epoch
        single pass through the training set

# https://www.coursera.org/learn/deep-neural-network/lecture/lBXu8/understanding-mini-batch-gradient-descent
    stochastic gradient descent
        mini batch size == 1
        
    typical batch size
        2**n
            64 ~ 512
            
# https://www.coursera.org/learn/deep-neural-network/lecture/duStO/exponentially-weighted-averages
    moving average
        exponentially weighted average, 
        reason it called "exponentially", is because the "feedback" structure, exponential filter
        
        
        
        
# https://www.coursera.org/learn/deep-neural-network/lecture/y0m1f/gradient-descent-with-momentum
    using exponentially weighted average to build a better optimization algorithm
        calculating the exponentially weight of the gradient, 
        use this gradient to update the gradients
        
    reduce the speed onsome specific direction
    
    momentum, 
        # df: use the previous speed direction as component of current one, once find a direction, keep go along it for some extent
        here is using average to smooth directions,
            so that direction with more ocileration is slown down
            the direction with less ocilleration is fasteren up
            
            
# https://www.coursera.org/learn/deep-neural-network/lecture/BhJlm/rmsprop
    RMSprop
        root mean squre properation
        not using the S_w directly
        
        
# https://www.coursera.org/learn/deep-neural-network/lecture/w9VCZ/adam-optimization-algorithm
    adam
        adaptive moment estimation
            = momentum + RMSprop
                # divide the momentum by the RMSprop
        
        
# https://www.coursera.org/learn/deep-neural-network/lecture/RFANA/the-problem-of-local-optima
    momentum
    RMSprop
    Adam
        are all for speed up the algorithm, by allowing to use the big learning rate
        which is important to overcome the plateau problem
        
        
# https://www.coursera.org/learn/deep-neural-network/lecture/3rdqN/using-an-appropriate-scale-to-pick-hyperparameters
    sample in log scale
        r = -4 * np.random.uniform()
        alpha = 10**r
        
        
# https://www.coursera.org/learn/deep-neural-network/lecture/4ptp2/normalizing-activations-in-a-network
    batch normalization
        # local nomalization?
        make it easier for hyperparameters search
        
        not for normalization a mini batch
        instead it normalize the z
        
# https://www.coursera.org/learn/deep-neural-network/lecture/RN8bN/fitting-batch-norm-into-a-neural-network
    reason called batch norm
        using with the mini batch, instead of the batch gradient
        but in fact no difference of the mechanism, only using the different mean and variance
        
        
        
        
# 29/05/2018
# https://www.coursera.org/learn/deep-neural-network/lecture/81oTm/why-does-batch-norm-work
    batch norm reduce the previous layers parameters changes leading to the current activation units change too much, 
    then the performance of the NN will be impacted, as the new distribution is not familiar to the NN
    
    boost the independence between different layers, speed up the learning speed? how?
        # so as to have bigger slope of the g

    add noise to hidden layers, make the dependence weaker, not strongly rely on some specific hidden unit,
    thus slight regularization effect
    
        as bigger mini batch size lead to smaller noise on mean and var,
        thus wierd:
            bigger batch weaker regularization effect


# https://www.coursera.org/learn/deep-neural-network/lecture/FsoNw/batch-norm-at-test-time
    problem with my TP.predict
        using half of the batch normalization
        but not applying the normalization to the prediction?0
        
        
        
# https://www.coursera.org/learn/deep-neural-network/lecture/HRy7y/softmax-regression
    why not just trivial logistic regression, and normalize among all the output units
    softmax is function of vector, instead of sigle value like ReLU or logistic regression


# https://www.coursera.org/learn/deep-neural-network/lecture/LCsCH/training-a-softmax-classifier
    hardmax
        max with 1,
        others with 0
        
    when c == 2, softmax == logistic
        
    softmax loss function does not use (1-y)log y  to deal with the negative example, 
    is because, there is the constraint on sum(Y) = 1, as long as the prob corresponding to 1
    is big, the prob corresponding to 0 is small




# 
XOR XOR XOR ...
    2**(n-1)
    due to n-1 * XOR, can be 0/1
    then 2**(n-1) sequence of "111011101101"(or) union together to enumerate all the possible sequence
    of XOR sequence
    
    
# https://www.coursera.org/learn/machine-learning-projects/lecture/LG12R/avoidable-bias
    by definition, nothing can be better than the Bayes error, unless overfitting
    
    avoidable bias
    variance
        tools for analysising what kind of problem should be focus on,
        i.e. the one has more space for improvment
        
        
# https://www.coursera.org/learn/machine-learning-projects/lecture/IGRRb/cleaning-up-incorrectly-labeled-data
    systematic errors
    





# https://www.coursera.org/learn/machine-learning-projects/lecture/WNPap/transfer-learning
    pre-training 
    fine tunning
    
    why transfer learning minght not hearts?
        what if it already stuck on a pleatue by pre training, and fine tunning could not get it out?
    
    # more popular!
    
    
# https://www.coursera.org/learn/machine-learning-projects/lecture/l9zia/multi-task-learning
    the main difference between softmax and multi-task
        multi-task is using the combination of logistic regressions,
            1 image multi-labels
            allow multiple 1 in output label
            when the images are not fully labeled, denote the unchecked label dimension with "?",
            then at the final output we can just skip the "?"s when calculate the loss  
        softmax
            1 image sigle-label
            only allow sigle 1 in output label
            
            
both transfer learning and multi-task learning, it seems need much more auxiliary data than the explicit data,
why? will not lead to bias??
            
            
            
# https://www.coursera.org/learn/machine-learning-projects/lecture/k0Klk/what-is-end-to-end-deep-learning
    end to end learning
        is using one NN directly predict from a image to the label
        
        simple system structure, direct mapping
        need a lot of data
        
    divide up, into steps
        need much less data
    
    
    
    
    
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/o7CWi/padding
    padding
        is for solve the problems:
            1. shrinking output
            2. throw away info from edges
            
        valid
            no padding
        same
            maintain the same size between input image and output image
            
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/wfUhx/strided-convolutions
    stride convolution
        means each step shift not only one element
        
    
    
    
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/A9lXL/simple-convolutional-network-example
    general trend of convolutional network
        1. n_h n_w increasing
        2. n_c descreasing
        
    3 types of layers
        1. convolution
        2. pooling
        3. fully connected
        
        
        
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/hELHk/pooling-layers
    max pooling
        normal sliding window, but no params, just get the max
            # "max" feature
        emphasis on some particular feature
        
        used more often
        
    average pooling
        use to collapse the representation
        
    all poolings have no parameters to learn
        
        
        
        
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/uRYL1/cnn-example
    as pool layer does not have weights, 
    conventionally treat it as the same layer with the conv layer
        
    full connection layer
        normal NN layer
        
        units in adjecent layers are fully connected
        on the contrary, the CONV layers, the features only related to the features of the previous layer,
        which inside the sliding window
        
        
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/Xv7B5/why-convolutions
    conv uses much less params then FC
        due to 
            1. sliding window - parameter sharing
            2. only connect to sliding window
        
        translation invariance
            robust
            
            
"parameters"
    conv
        300 * 300 * 3
        5 * 5 * 3 + 1
            parameters for one filter, need to be 3-D
        (5 * 5 * 3 + 1) * 100
        
    NN
        300 * 200 + 300
            300 weights for the bias 1 : b_1 ... b_300
        parameters 100 filters
        
        
        
        
        
        
# 31/05/2018
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/HAhz9/resnets
    residual network
        skip connection / short cut
            a[l] skip a[l+1] and sum with z[l+1] directly
            
            

        
        
        
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/5WIZm/inception-network-motivation
    error on 2:06
        pooling conduct on each channel independently, does not change the depth of the image
        other thing is just like the normal convolution sliding window
        
        
        
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/5WIZm/inception-network-motivation
    bottleneck layer
        use the 1*1 filter to shrink the image firstly
        
        this process will not lossing the information?
        
        
        
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/4THzO/transfer-learning
    transfer learning
        freeze some layers
        train some layers
            based on how many data you have
            
            
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/AYzbX/data-augmentation
    not all machine learning app need a lot of data
    but computer vision does, cannot get enough data
    
    
    
# https://www.coursera.org/learn/convolutional-neural-networks/lecture/nEeJM/object-localization
    localization
        1 object in the fig
    detection
        multi objects in the fig
        
    put the pc, x,y, dh, dw, dx, dy into the y,
    which in fact is a combination of classification and regression problem.
        the loss is in different shape when pc has different value.
        "potential problem?:"
            only compare pc when pc == 0
            |y^, y| when pc != 0,
                is if possible, the |y^,y| is bigger than the |pc^, pc|
                thus, cost put more weight on the |y^,y|, 
                so that has worse performance on |pc^,pc|?
        
        
        
        
        
