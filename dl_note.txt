# 28/05/2018

# https://www.coursera.org/learn/deep-neural-network/lecture/XzSSa/numerical-approximation-of-gradients
two side approximation is more accurate than the one side approx

# https://www.coursera.org/learn/deep-neural-network/lecture/htA0l/gradient-checking
gradient check
    reshape the parameters 
    d_parameters 
        into big vector
        do numercial approx 
        on each dimension
        to get theta_approx
        
    check the approximationality between theta_approx and theta
        use cos similarity
            normalization with the length of theta_approx and theta
            
https://www.coursera.org/learn/deep-neural-network/lecture/6igIc/gradient-checking-implementation-notes
    with drop out the J is different as the network is different
    then, the gradient checking technique is not suitable to compute here
    so, do gradient checking first, when find no problem, turn on the drop out
    
xavier initialization
    used for making the wx not too big or too small, so that the g(wx) has the big slope, then make the optimazation faster,
        tanh : np.sqrt(1 / n)
            # xavier
        ReLU : np.sqrt(2 / n)

    
# https://www.coursera.org/learn/deep-neural-network/lecture/qcogH/mini-batch-gradient-descent
    an epoch
        single pass through the training set

# https://www.coursera.org/learn/deep-neural-network/lecture/lBXu8/understanding-mini-batch-gradient-descent
    stochastic gradient descent
        mini batch size == 1
        
    typical batch size
        2**n
            64 ~ 512
            
# https://www.coursera.org/learn/deep-neural-network/lecture/duStO/exponentially-weighted-averages
    moving average
        exponentially weighted average, 
        reason it called "exponentially", is because the "feedback" structure, exponential filter
        
        
        
        
# https://www.coursera.org/learn/deep-neural-network/lecture/y0m1f/gradient-descent-with-momentum
    using exponentially weighted average to build a better optimization algorithm
        calculating the exponentially weight of the gradient, 
        use this gradient to update the gradients
        
    reduce the speed onsome specific direction
    
    momentum, 
        # df: use the previous speed direction as component of current one, once find a direction, keep go along it for some extent
        here is using average to smooth directions,
            so that direction with more ocileration is slown down
            the direction with less ocilleration is fasteren up
            
            
# https://www.coursera.org/learn/deep-neural-network/lecture/BhJlm/rmsprop
    RMSprop
        root mean squre properation
        not using the S_w directly
        
        
# https://www.coursera.org/learn/deep-neural-network/lecture/w9VCZ/adam-optimization-algorithm
    adam
        adaptive moment estimation
            = momentum + RMSprop
                # divide the momentum by the RMSprop
        
        
# https://www.coursera.org/learn/deep-neural-network/lecture/RFANA/the-problem-of-local-optima
    momentum
    RMSprop
    Adam
        are all for speed up the algorithm, by allowing to use the big learning rate
        which is important to overcome the plateau problem
        
        
# https://www.coursera.org/learn/deep-neural-network/lecture/3rdqN/using-an-appropriate-scale-to-pick-hyperparameters
    sample in log scale
        r = -4 * np.random.uniform()
        alpha = 10**r
        
        
# https://www.coursera.org/learn/deep-neural-network/lecture/4ptp2/normalizing-activations-in-a-network
    batch normalization
        # local nomalization?
        make it easier for hyperparameters search
        
        not for normalization a mini batch
        instead it normalize the z
        
# https://www.coursera.org/learn/deep-neural-network/lecture/RN8bN/fitting-batch-norm-into-a-neural-network
    reason called batch norm
        using with the mini batch, instead of the batch gradient
        but in fact no difference of the mechanism, only using the different mean and variance
        
        
        
        
# 29/05/2018
# https://www.coursera.org/learn/deep-neural-network/lecture/81oTm/why-does-batch-norm-work
    batch norm reduce the previous layers parameters changes leading to the current activation units change too much, 
    then the performance of the NN will be impacted, as the new distribution is not familiar to the NN
    
    boost the independence between different layers, speed up the learning speed? how?
        # so as to have bigger slope of the g

    add noise to hidden layers, make the dependence weaker, not strongly rely on some specific hidden unit,
    thus slight regularization effect
    
        as bigger mini batch size lead to smaller noise on mean and var,
        thus wierd:
            bigger batch weaker regularization effect


# https://www.coursera.org/learn/deep-neural-network/lecture/FsoNw/batch-norm-at-test-time
    problem with my TP.predict
        using half of the batch normalization
        but not applying the normalization to the prediction?0
        
        
        
# https://www.coursera.org/learn/deep-neural-network/lecture/HRy7y/softmax-regression
    why not just trivial logistic regression, and normalize among all the output units
    softmax is function of vector, instead of sigle value like ReLU or logistic regression


# https://www.coursera.org/learn/deep-neural-network/lecture/LCsCH/training-a-softmax-classifier
    hardmax
        max with 1,
        others with 0
        
    when c == 2, softmax == logistic
        
    softmax loss function does not use (1-y)log y  to deal with the negative example, 
    is because, there is the constraint on sum(Y) = 1, as long as the prob corresponding to 1
    is big, the prob corresponding to 0 is small




# 
XOR XOR XOR ...
    2**(n-1)
    due to n-1 * XOR, can be 0/1
    then 2**(n-1) sequence of "111011101101"(or) union together to enumerate all the possible sequence
    of XOR sequence
    
    
# https://www.coursera.org/learn/machine-learning-projects/lecture/LG12R/avoidable-bias
    by definition, nothing can be better than the Bayes error, unless overfitting
    
    avoidable bias
    variance
        tools for analysising what kind of problem should be focus on,
        i.e. the one has more space for improvment
        
        
# https://www.coursera.org/learn/machine-learning-projects/lecture/IGRRb/cleaning-up-incorrectly-labeled-data
    systematic errors
    

        
